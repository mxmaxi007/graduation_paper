\thusetup{
  %******************************
  % 注意：
  %   1. 配置里面不要出现空行
  %   2. 不需要的配置信息可以删除
  %******************************
  %
  %=====
  % 秘级
  %=====
  secretlevel={秘密},
  secretyear={10},
  %
  %=========
  % 中文信息
  %=========
  ctitle={语音情感识别中特征学习和选择的研究},
  cdegree={工程硕士专业},
  cdepartment={计算机科学与技术系},
  cmajor={计算机技术},
  cauthor={马习},
  csupervisor={贾珈副教授},
  % csupervisor={吴志勇\hspace*{24pt}副研究员},
  % % cassosupervisor={陈文光教授}, % 副指导老师
  % % ccosupervisor={某某某教授}, % 联合指导老师
  % % 日期自动使用当前时间，若需指定按如下方式修改：
  % % cdate={超新星纪元},
  % %
  % % 博士后专有部分
  % cfirstdiscipline={计算机科学与技术},
  % cseconddiscipline={系统结构},
  % postdoctordate={2009年7月——2011年7月},
  % id={编号}, % 可以留空： id={},
  % udc={UDC}, % 可以留空
  % catalognumber={分类号}, % 可以留空
  %
  %=========
  % 英文信息
  %=========
  etitle={Feature Learning and Selection for Speech Emotion Recognition},
  % 这块比较复杂，需要分情况讨论：
  % 1. 学术型硕士
  %    edegree：必须为Master of Arts或Master of Science（注意大小写）
  %             “哲学、文学、历史学、法学、教育学、艺术学门类，公共管理学科
  %              填写Master of Arts，其它填写Master of Science”
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 2. 专业型硕士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：“工程硕士填写工程领域，其它专业学位不填写此项”
  % 3. 学术型博士
  %    edegree：Doctor of Philosophy（注意大小写）
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 4. 专业型博士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：不填写此项
  edegree={Master of Engineering},
  emajor={Computer Technology},
  eauthor={Ma Xi},
  esupervisor={Associate Professor Jia Jia},
  % esupervisor={Associate Professor Wu Zhiyong},
  % eassosupervisor={Chen Wenguang},
  % 日期自动生成，若需指定按如下方式修改：
  % edate={December, 2005}
  %
  % 关键词用“英文逗号”分割
  % ckeywords={\TeX, \LaTeX, CJK, 模板, 论文},
  % ekeywords={\TeX, \LaTeX, CJK, template, thesis}
}

% 定义中英文摘要和关键字
\begin{cabstract}
  在人机交互日益普遍的今天，只是理解语音中的语言学信息已经不足以满足所有需求，如何提取语音中的情感信息在许多的应用场景中变得越来越重要。但是如何为不同情感更有效地抽取和选择特征并没有被现有的研究广泛关注。针对这个问题，本文分别从传统的语音情感识别方法和端到端的深度学习方法入手，设计对应的方法来抽取和选择特征，以提升系统的情感识别准确率。主要的研究工作和和贡献如下：

  % 传统的语音情感识别大致可以分为情感相关声学特征的提取和情感分类模型的构建两部分，原始语音信号通常会先被映射到情感信息相关的声学特征，然后采用各种分类模型将特征向量映射到对应的情感类别。近年来，随着深度学习方法的发展与普及，深度神经网络开始越来越多被应用到语音情感识别，并且取得了不错的效果。此外，特征提取和情感分类两个部分也开始被整合到一起，通过深度神经网络将可以构建从原始语音信号直接到情感类别的端到端的识别系统，
 
\textbf{一、提出了一种基于情感对的语音情感识别框架，为不同的情感对选择不同的声学特征，并在最后的决策融合过程中引入心理学的情感空间模型，提升系统的识别准确率。} 传统的语音情感识别系统通常为所有的情感选取相同的声学特征来完成最后的情感分类，但实验证明不同的情感和不同的声学特征的相关性并不同。针对这一问题，我们分别为不同的情感对选取不同的特征集合，将原先的多分类问题转变为多个二分类问题，并在最后的决策融合过程中通过贝叶斯分类器引入情感空间的信息。
% 在公开的情感语音数据集IEMOCAP上，我们方法取得了比传统的识别框架更好的准确率。

\textbf{二、构建了基于深度神经网络的端到端的语音情感识别系统，使用语谱图代替传统的声学特征，从而提升情感识别准确率。} 随着深度学习技术和工具的发展，许多的研究者开始采用深度神经网络在原始语音信号上直接构建分类或者回归模型，被称之为端到端的系统。相比于传统的声学特征，这种方法可以抽取到更符合任务目标的特征表示。语谱图是语音信号的一种无损表示，我们通过卷积神经网络从语谱图上直接抽取和情感相关的特征表示，然后通过循环神经网络来建模语音信号的时序信息，最后通过全连接网络将输出映射到不同情感的后验概率。
% 相比于传统的语音情感识别，端到端的系统能够取得更好的准确率。

\textbf{三、设计了一种能够处理变长语音段的神经网络结构来实现端到端的系统，消除了语音分段带来的中性语音和情感语音的混淆，进一步提升了情感识别的准确率。} 在使用深度神经网络实现端到端的语音情感识别系统时，由于模型很难处理变长的输入，通常会把变长的语音句子切分成多个等长的语音段，然后将所有语音段都标记为对应句子的情感标签，但这样会导致部分中性语音段被标记为有情感。针对于这一问题，我们采用补齐和掩码的方式来处理神经网络中变长的输入序列，保证模型可以接受整个语音句子，从而避免了错误标注带来的效果变差。
% 相对于切分等长语音段的方法，我们直接输入整个变长语音句子的方法可以在相同的数据集上取得更好的准确率。
 
\end{cabstract}

% 如果习惯关键字跟在摘要文字后面，可以用直接命令来设置，如下：
\ckeywords{语音情感识别；情感对；维度情感空间模型；端到端深度神经网络；变长语音段}

\begin{eabstract}
  Nowadays human-machine interaction is becoming more and more popular. The machine only understands linguistic information in speech, which is not enough to satisfy all needs. How to extract emotional information in speech is becoming more and more important in many application scenarios. However, how to extract and select features more effectively for different emotions has not been widely studied. In order to solve this problem, this paper will design the corresponding methods to extract and select features with traditional speech emotion recognition and end-to-end speech emotion recognition, and then improve the recognition rate of the system. The main contributions of this paper include:

  % Traditional speech emotion recognition can be divided into two parts: acoustic feature extraction and emotion classification model. The original speech signals are usually mapped to the acoustic features, and then use the classification models to map the features into the corresponding emotional categories. In the recent years, with the development of deep learning, deep neural networks have been introduced to speech emotion recognition, and have achieved good results. Furthermore, feature extraction and emotion classification are beginning to the integrated together. Through deep neural network, the end-to-end recognition system can be builded from original speech signals to emotional categories.

\textbf{1. A emotion-pair based speech emotion recognition framework is proposed to select different features for different emotion-pairs, and the dimensional emotion space is introduced in the decision fusion, thus improving the recognition rate of the system.}  In general, the traditional speech emotion recognition system select the same acoustic features for all emotions to complete the emotion classification, but the studies have shown that the correlation between different emotions and different acoustic features is different. To solve this problem, we will select different acoustic features for different emotions, and turn the multi-classification problem into bi-classification problems. In the decision fusion stage, the information of emotional space is introduced by Bayes classifier. 
% In the IEMOCAP emotional speech dataset, our method can achieve better accuracy than the traditional recognition framework.

\textbf{2. The deep neural network based end-to-end speech emotion recognition system is constructed, which uses the spectrogram instead of the acoustic features, thus improving performance of emotion recognition.}  With the development of deep learning techniques and tools, many researchers have begun to use deep neural network to directly construct classification and regression models on original speech signals, the so-called end-to-end system. Compared with traditional acoustic features, this method can extract feature representations which are more related to the target. Spectrogram is a lossless representation of speech signals. We use the convolution neural networks to extract the emotion related features directly from the spectrogram,  and then model the time sequence information of the speech signals through recurrent neural networks, and the map the output to the posterior probability of the different emotions through the full-connected neural networks. 
% Compared to the traditional speech emotion recognition, the end-to-end system can achieve better accuracy.

\textbf{3. A neural network structure which can handle the variable-length speech segments is designed to implement the end-to-end system. This structure can relieves the confusion between neutral speech and emotional speech brought by the speech segmentation, thus further improving the recognition rate of the system.} When using deep neural networks to implement the end-to-end speech emotion recognition system, because the model is difficult to deal with the variable-length input, the speech sentences are usually divided into multiple equal-length speech segments, and all the speech segments are labeled as the emotional labels of the corresponding sentence. However, this causes some neutral speech segments to be marked with other emotions. In order to solve this problem, we use the method of padding and masking to deal with the variable-length input sequence in the neural networks, and ensure that the model can accept the whole speech sentence. This will avoid the performance degeneration through introducing the wrong labels. 
% Compared with the method of segmenting equal-length speech segments, we can get better accuracy on the same dataset by directly inputting the whole variable-length speech sentence.

\end{eabstract}

\ekeywords{Speech Emotion Recognition; Emotion-Pair; Dimensional Emotion Space; End-to-End Deep Neural Network; Variable-Length Speech Segments}
