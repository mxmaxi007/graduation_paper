\chapter{基于情感对的语音情感识别框架}
\label{cha:emo_pair_base_framework}

\section{本章引论}
\label{sec:emo_pair_base_framework_intro}
特征选择作为传统语音情感识别中一个重要的部分，已经吸引了许多的研究者的关注。因为情感是人类的主观感受，想要通过声音中的线索来反应当前说话人的情感状态是一个非常具有挑战性的任务。目前大多数的研究都旨在为所有的情感类别找到一个统一的特征集合，因为这也是通常处理多分类问题的方法，在一个共同的特征集合上构建分类器。但是一些研究结果已经证明与不同的情感相关的声学特征也是不同的，也就是说为所有的情感识别选择相同的特征集合并不是一个很好的方法，为特定的情感选择特定的特征可以取得更好的效果。

基于上述原因，我们认为给不同的情感组合选择相关程度最高的的特征子空间，保证这些情感在这样的特征子空间上具有更高的可分性，是一个比较适合的实现方式。为此，我们提出了一种基于情感对的语音情感识别框架。我们将任意两种不同的情感组成情感对，然后为每一个情感对选择最相关的声学特征子集。这种特征选择方式将在很大程度上缩减需要处理的问题域，因为现在我们将之前需要为多种情感选择特征的问题转换为为两种情感选择特征的问题，剔除了许多无关的干扰特征。当为每一个情感对选择出对应的特征子集之后，我们将在每个特征子集上构建二分类器，这样就将原本的多分类问题转换为多个二分类问题。此时，又存在一个问题，就是最后我们期望的结果是一句话只得到一个识别结果，但现在每个二分类器都会得到一个结果，所以我们还需要再加入一个决策融合的步骤。最简单的决策融合方法就是采用投票策略，将所有二分类器的结果中出现次数最多的那个情感作为最终的识别结果。但投票的策略存在两个缺点，一是会出现票数相同的问题，二是只有和目标情感相关的那些二分类结果才有贡献，其他的二分类结果只会产生干扰。为了避免这两个缺点，我们引入情感空间模型中不同情感之间的距离信息，通过贝叶斯分类器来完成决策融合的步骤，整个系统的流程图如图1所示。

基于层次的语音情感识别框架和我们的设计有着相似的思想，都是期望为不同的情感类别采用不同的特征集合，但是我们的方法效果更好，而且当情感类别发生变化时更容易扩展。此外，由于多个二分类任务之间没有依赖关系，所以更易于并行加速。

本章的剩余部分是这样安排的：首先我们将介绍情感对的定义，基于情感对的特征选择算法，和基于情感对的二分类模型；然后，我们将介绍决策融合的方法，包括基于投票的决策融合和基于情感空间的决策融合；最后，我们将和采用全局特征集合的方法，和基于层次分类的方法进行实验对比。

\begin{figure}[H] % use float package if you want it here
    \centering
    \includegraphics[height=10cm]{myfigures/2dim_space}
    \caption{激活度-效价情感空间模型}
    \label{fig:xfig1}
\end{figure}

\section{情感对}

\subsection{情感对的定义}
任意两个不相同的情感组合在一起就被称为情感对，这种思想是源于集成学习(Ensemble Learning)的一些观点。集成学习大致可以分为两种方式，一种被称为引导聚合(Bootstrap Aggregating, Bagging)，这种方法利用重采样的方法从整体数据集中采取有放回抽样得到N个数据集，在每个数据集上学习出一个模型，最后的预测结果利用N个模型共同决策得到，例如随机森林(Random Forest)就属于这种方法。总的来说，引导聚合就先训练多个简单的弱分类器，然后通过这些弱分类器组合成一个强分类器。另一种被称为提升方法(Boosting)，这种方法是一种可以用来减小监督学习中偏差的机器学习算法。主要也是学习一系列弱分类器，并将其组合成一个强分类器，其中最具代表性的方法是AdaBoost(Adaptive Boosting)算法，这种算法在刚开始训练时对每一个训练样例赋相同的权重，然后对训练集进行多轮迭代训练，每轮训练结束后都对预测错误的那些样例赋以较大的权重，也就是让学习算法以后更注意学错的样本，从而可以得到多个预测模型，最终再以正比于准确率的权重将所有模型的结果组合到一起得到最后的结果。

我们提出的情感对有些类似于引导聚合的思想，我们将为不同的情感对分别选择特征和训练分类器，最后再将所有分类器的结果融合到一起。由于我们只需要为两种情感选择对应的特征，相比于为所有的情感选取相关的特征，这大大减少了干扰特征的引入。和我们有相似思想的还有基于层次的语音情感识别方法，这种方法首先会根据观察设计一棵二叉决策树，在树中的不同节点分别区分不同的情感类别或情感类别组，并且每一个节点的分类器都是单独选择特征集的。情感类别组是指将多种情感分别归属到两个不同的组中，然后将这两个组看做两个类。识别过程是语音从根节点开始进行分类，然后自顶向下沿着路径上的节点依次进行分类，最后到达叶子节点后将会得到一种唯一的情感，整个分类流程如图2所示。但这个方法有两个缺点，第一是整个决策树的结构需要人工来设计，当情感类别或应用场景发生变化时，整个决策树就得重新设计，并不具备通用性。第二是由于整个分类过程是自顶向下的，所以会存在错误累计的问题，就是说如果上一层的分类结果是错误的，这些错分的样本就会沿着路径一直走下去，从而影响下层节点的分类效果。我们基于情感对的方法则可以解决这两个问题，首先情感对的方法不需要任何的人工介入，因为情感对只是将任意两种情感组合到一起，即使需要加入新的情感类别，仅仅只是需要生成新的情感对，之前已经训练好的情感对模型仍然可以使用，这将大大减少训练时间。其次，情感对的方法不存在错误累计问题，因为不同的情感对之间的输入都是独立的，不会相互产生影响。此外，由于不同情感对之间没有依赖关系，所以训练可以并行完成，这将进一步提升训练速度。

\begin{figure}[H] % use float package if you want it here
    \centering
    \includegraphics[height=10cm]{myfigures/2dim_space}
    \caption{激活度-效价情感空间模型}
    \label{fig:xfig1}
\end{figure}

下面两小节我们将专门针对基于情感对的声学特征选择算法和二分类模型做出详细的阐述，这两个问题是整个识别框架中最关键的问题。

\subsection{基于情感对的声学特征选择}
特征选择选择算法有许多，这些算法的目的主要可以分为三个方面：第一是提升模型的预测准确率；第二是筛除无用的冗余特征，减少计算时间；第三是针对于当前的问题域，对于数据提供一个更好的理解。下面将会对特征选择指标，特征选择策略以及基于情感对的特征选择实现这三个方面分别作介绍。

\subsubsection{特征评价指标}
对于特征选择算法来说最重要实现选取一种指标来衡量选出的特征的好坏。假设我们有一个含有$m$个样例的数据集$\{\mathbf{x_k}, y_k\}(k=1,...,m)$，每个样例包含$n$个输入特征$x_{k,i}(i=1,...,n)$和一个输出值$y_k(k=1,...,m)$，第$i$个特征的评判指标通过函数$S(i)=f(\{x_{k,i}, y_k\}(k=1,...,m))$来定义。一般来说，我们定义$S(i)$越大的特征和任务目标越相关。为了方便下面的解释，我们再引入一些其他的定义：如果输入向量$\mathbf{x}$被看做来自一个潜在的多变量概率分布$\mathbf{X}$，我们定义$X_i$代表$\mathbf{x}$中第$i$个特征的随机变量。同样，$Y$代表输出值$y$的随机变量。进一步，我们定义$m$维的向量$\mathbf{x_i}$代表数据集中第$i$个特征的所有值，$m$维的向量$\mathbf{y}$代表数据集中所有样本的输出值。

第一种特征评价指标叫做相关系数，我们假设输出$y$是一个连续值，则泊松相关系数被定义为：
\begin{equation}
\label{equ:pearson_cc}
    \Re(i) = \frac{cov(X_i, Y)}{\sqrt{var(X_i)var(Y)}}
\end{equation}
$cov$代表协方差，$var$代表方差。通过统计方法对$\Re(i)$的估计$R(i)$可以表示为：
\begin{equation}
\label{equ:pearson_cc_real}
    R(i) = \frac{\sum_{k=1}^{m}(x_{k,i} - \overline{x_i})(y_k - \overline{y})}{\sqrt{\sum_{k=1}^{m}(x_{k,i} - \overline{x_i})^2}\sum_{k=1}^{m}(y_k - \overline{y})^2}
\end{equation}
符号上面的横线代表对所有的下标$k$求平均值，这个系数也可以看作向量$\mathbf{x_i}$和向量$\mathbf{y}$的余弦值。在线性回归中，评价指标通常是$R(i)$的平方，这样将可以去除负值，用来表示向量$\mathbf{x_i}$和向量$\mathbf{y}$的线性相关性。但是相关系数$R(i)$仅仅可以表示特征和目标之间的线性依赖关系，如果希望可以获取非线性关系，最简单的方法就是对输入进行非线性的处理。例如取平方，开方，取对数等等。

第二种方法是采用单变量分类器，这种方法是将一个特征输入一个分类器中，然后将分类器的预测能力作为衡量指标。通常用来衡量分类器的预测能力的指标是错误率，例如对于二分类问题，错误的识别为正例的比率(Fasle Positive Rate, FPR)和错误识别为负例的比率(False Negative Rate, FNR)都可以被被定义为衡量标准，但通常为了平衡两种错误率，会选择ROC曲线(Receiver Operating Characteristic Curve)下区域的面积，也就是AUC(Area Under Curve)作为衡量的指标。

第三种方法是利用信息学理论中的一些指标，最常使用的是变量和目标之间的互信息量，定义如下：
\begin{equation}
\label{equ:mutual_info}
    I(i) = \int_{x_i}\int_{y}p(x_i, y)log\frac{p(x_i, y)}{p(x_i)p(y)}dydx_i
\end{equation}
其中$p(x_i)$和$p(y)$是$x_i$和$y$的概率密度，$p(x_i, y)$是联合概率密度，$I(i)$是用来衡量变量$x_i$的概率密度和变量$y$的概率密度之间的相关性。连续变量的概率密度$p(x_i)$，$p(y)$和$p(x_i, y)$都是未知的，并且很难从数据集中估计出来。通常在估计连续变量的概率密度时，会先假设变量服从某种已知的概率分布，例如高斯分布，然后通过数据集中变量的统计值来估计这种分布的参数。相对而言，离散变量的概率分布是更容易估计的，因为积分可以通过求和来替代，计算公式如下：
\begin{equation}
\label{equ:mutual_info_discrete}
    I(i) = \sum_{x_i}\sum_{y}P(X=x_i, Y=y)log\frac{P(X=x_i, Y=y)}{P(X=x_i)P(Y=y)}
\end{equation}
公式中的概率可以通过统计不同值出现的的频数。例如，在一个三分类的问题中，输入一共有4个特征，$P(Y=y)$代表类别的先验概率(3种可能)，$P(X=x_i)$代表输入特征的分布(4种可能)，$P(X=x_i, Y=y)$代表联合概率(12种可能)，但是当类别和特征的数量增多时，这种估计也将变得更加困难。

\subsubsection{特征选择策略}
上一章我们列举了衡量特征的指标，但是上面的指标主要是针对于单个特征来说的。通常我们选择特征时会选择一个特征子集，并不是说特征子集中所有单个的特征指标最好就代表特征子集是最好的。因为一些研究已经证明，不同特征组合在一起时，相互之间会产生影响。一些评价指标比较低的特征可能刚好补充了其他特征缺失的那一部分信息，一些评价高的特征也有可能和其他特征所包含的信息有重复，所以我们需要通过一些方法来保证能够选择到好的特征子集。特征子集选择的策略大致可以分为两类，一类叫作打包(Wrappers),它是通过预测模型在采用不同特征子集时的预测能力来评价的；另一类叫作过滤(Filters)，是通过一些预处理步骤得到特征子集，与预测模型无关，下面分别介绍两类方法。

打包(Wrappers)是一种简单和有效的特征选择策略。在通常情况下，打包(Wrappers)通过预测模型的预测能力来评价特征子集的有效性，所以需要解决的有三个问题：一是如何能够遍历所有的特征子集，二是如何通过预测模型的预测能力指导遍历过程，三是应该选择哪种预测模型。当特征数量不是太多的时候，完全遍历将会被采用。但完全遍历本身是一个NP-hard问题，当特征数量增多时，完全遍历的计算量是难以承受的，一些启发式的遍历算法被提出来降低计算量，期望得到一个近似最佳的结果，例如局部贪心，剪枝搜索，模拟退火，遗传算法等。预测能力则可以使用评价指标中提到的错误率来衡量。预测模型的选择没有太多的理论指导，通常都是根据经验来选取。由于特征选择的过程和选择的预测模型是相关的，因此大多数人通常会将特征选择和模型训练作为一个整体来完成。

过滤(Filters)是另一种特征选择的策略，这种策略不需要预测模型的参与，而是利用一些信息学理论中的指标，仅仅在数据预处理的阶段就能够完成。相比于打包(Wrappers)的策略，一些研究者认为过滤(Filters)的策略有两个主要的优点：一是由于不涉及模型训练，所以处理速度更快；二是这种方法并不针对于某种特定的模型，所以选出的特征更有普适性，在所有的预测模型中都可以使用，不易产生过拟合。这种算法主要利用互信息量这种评价指标，相关的算法比较少，比如马尔科夫毯(Markov Blanket)。

除了上面提到的特征子集选择以外，还有一些特征空间压缩的方法具有相同的效果。特征空间压缩是指将原始的特征空间通过某种函数关系映射到更低维的特征空间，进而提升效果并减少计算量。此外，构建新的特征空间可以帮助我们更好地理解所处理的问题。特征空间压缩的算法有很多，包括聚类方法，线性转换，
小波变换，卷积核等等，下面我们主要介绍聚类和矩阵变换这两类方法。

聚类方法被广泛地用在特征重构上面，这种方法的思想是将一组相似的特征用他们的聚类中心所替代，变成一个新的特征，具体过程如图所示。最流行的算法包括K均值(K-means)和分层聚类。聚类通常是一种无监督的算法，但也可以引入一些监督信息来得到更加有效的特征。假设$\widetilde{X}$是一个代表重构特征的随机变量，$X$代表原有的特征，$Y$代表预测目标。监督式算法的目的是在保证$\widetilde{X}$和$Y$的互信息量$I(\widetilde{X}, Y)$同时，最小化$X$和$\widetilde{X}$的互信息量$I(\widetilde{X}, X)$。这可以通过引入拉格朗日算子$\lambda$来构建全局目标函数:
\begin{equation}
\label{equ:lagrange_multiplier}
    J=I(\widetilde{X}, X) - \lambda I(\widetilde{X}, Y)
\end{equation}
这使得在搜索最大可能的压缩解的同时又可以保证对目标的潜在信息。

\begin{figure}[H] % use float package if you want it here
    \centering
    \includegraphics[height=10cm]{myfigures/2dim_space}
    \caption{激活度-效价情感空间模型}
    \label{fig:xfig1}
\end{figure}

矩阵变换是另一种特征空间压缩的方法，常用方法有PCA，SVD，LDA等。这里我们简单解释一下这三个算法的原理，PCA的目的是保证新的特征之间的方差最大，这样可以最大程度保留原始特征的信息。SVD和PCA类似，目标是构成一组由原始特征通过线性组合得到的新特征，并且最大可能保证新的特征保留原始特征的信息，但SVD可以应用在行数和列数不等的矩阵。PCA和SVD都属于无监督的算法，但LDA是一种有监督的算法，它的目标函数是在保证类别内样本的方差尽可能小，类别间样本中心点的距离尽可能大。
 
\subsubsection{基于情感对的特征选择实现}
在对情感对选择特征子集的时候，我们的目的是从一个大的声学特征集合中选择出一个最能够区分当前情感对中两种情感的特征子集。这里我们采用了打包(Wrappers)的方法，就是将分类器的识别率作为我们筛选特征的指标。首先需要一种遍历算法得到所有可能的特征子集，如果采用全局遍历的话需要耗费太多的时间，所以我们采用了一种启发式的遍历算法序列浮动前向选择算法(Sequential Floating Forward Selection,SFFS)。

SFFS的算法原理是，假设我们选择的特征子集为$S$，初始时$S$为空，每轮迭代从总的特征集合$W$中选出一个子集$A$，使得$A$加入$S$后的的评价函数$J(S)$达到最优，然后再从$S$中选择一个子集$B$,使得$S$剔除$B$后的的评价函数$J(S)$达到最优。经过多轮迭代，评价函数$J(S)$达到我们设定的阈值后算法停止。SFFS被广泛的应用在许多的模式识别任务中，可以在可接受的时间范围内选择出近似最优的特征子集。我们将会为每个情感对都运行SFFS算法得到相关的特征子集，整个特征选择的流程图如下图所示：

\begin{figure}[H] % use float package if you want it here
    \centering
    \includegraphics[height=10cm]{myfigures/2dim_space}
    \caption{激活度-效价情感空间模型}
    \label{fig:xfig1}
\end{figure}

\subsection{基于情感对的二分类模型}

当我们得到每个情感对相关的声学特征集后，下面的任务就是为每个情感对训练二分类器。二分类器的算法有很多种，在论文中我们采用了两种简单的分类器模型：支持向量机(Support Vector Machine, SVM)和贝叶斯逻辑回归(Bayesian Logistic Regression, BLR)。

SVM是一种二分类模型，它的基本原理是在特征空间中寻找保证使数据点的间隔最大化的分类超平面，如图所示。理想情况下，所有的训练样本都是线性可分的，SVM则可以找到一个完美的分类面将两类数据区分。但大多数情况下，训练样本并不是线性可分的，所以需要其他的一些方法来保证SVM能够正常工作。一般有两种方法：软间隔(Soft Margin)和核函数(Kernel Function)。软边界通过在目标函数中引入松弛变量，从而解决由于个别离群点而导致得到的分类超平面偏移到不好的位置。核函数则是通过将原始数据点映射到给高维的特征空间，使得在低维空间线性不可分的数据点在高维空间变得线性可分。

\begin{figure}[H] % use float package if you want it here
    \centering
    \includegraphics[height=10cm]{myfigures/2dim_space}
    \caption{激活度-效价情感空间模型}
    \label{fig:xfig1}
\end{figure}

 BLR是一种通过假设变量服从某种分布的逻辑回归(Logistic Regression)模型。首先，让我们先回顾下普通的逻辑回归模型，假设我们有特征向量$\phi$，两个类别$C_1$和$C_2$，我们需要建模后验概率$P(C_i|\phi)(i=1,2)$，逻辑回归模型采用下面的公式来建模：
 \begin{equation}
\label{equ:lagrange_multiplier}
    P(C_i|\phi) = \sigma(w^T\phi)
\end{equation}
其中$w$代表我们需要学习的参数，$\sigma$代表sigmoid函数。通过最大似然的算法就可以学习到参数的值。普通的逻辑回归模型都不假设后验概率服从某种分布，这也导致在拟合参数时和训练数据是强相关的，容易出现过拟合的情况。而贝叶斯方法通常都会假设后验概率的分布情况，训练数据通常都是用来估计概率分布的参数，然后再用概率分布去估计类别的概率，这样可以减少过拟合的情况，但是如果数据不服从假设的分布，也会导致训练结果变差。BLR的目的就是将贝叶斯方法的概率分布假设引入逻辑回归的参数估计中，具体来说就是在模型训练时不再精确地估计参数$w$，而是假设参数$w$服从某种分布，然后转而去估计这种分布的参数，这可以通过拉普拉斯近似的方法来实现，具体的细节可以参考[]。通过这种方式就可以有效的避免过拟合的出现，尤其是在训练数据比较少的情况下。

\section{决策融合}
在前面一章，我们已经为所有情感对的选择了相关的特征集，并且得到了二分类结果，但我们的目标是只得到唯一的情感类别，所有还需要一个步骤来将这些情感对的结果进行汇总，也就是决策融合。决策融合的算法最早出现在集成学习(Ensemble Learning)领域，因为集成学习通常需要训练多个模型，然后通过决策融合将所有模型的结果整合到一起。决策融合大致可以分为两种：训练型和非训练型。训练型的决策融合就是需要通过训练来建立所有模型的结果和最终结果之间的映射关系，非训练型的决策融合则是通过某些代数规则来将所有模型的结果映射到最终结果。下面会介绍本文中采用的两种决策融合的方法：基于投票的决策融合和基于情感空间的贝叶斯决策融合。

\subsection{基于投票的决策融合}
每一个情感对包含两种情感，二分类器可以得到这两种情感中的一种。假设我们需要识别$M$种情感，则一共可以组成$C_M^2 = \frac{M \times (M - 1)}{2}$个情感对，同样我们也会得到这么多的二分类结果。在所有的二分类结果中，每一种情感出现的次数最多为$M-1$次。基于投票的决策融合就是将所有二分类结果中出现次数最多的那个情感判定为最终的识别结果，因为通常出现次数多代表语音中包含这种情感的信息最多，下这属于一种无需训练的决策融合。下面是算法描述：

\begin{algorithm}[H]
    \caption{投票决策算法}
    \label{alg:voting}
    
    \begin{algorithmic}[1]
        \Require %算法的输入， \hspace*{0.02in}用来控制位置，同时利用 \\ 进行换行
            \Statex $M$: 情感类别的数量 
            \Statex $E = \{e_i, i=1,2,...,M\}$: 情感类别的集合
            \Statex $R = \{r_{e_ie_j}|e_i \neq e_j; r_{e_ie_j}, e_i, e_j \in E\}$: 情感对的二分类结果

        \Ensure %算法的结果输出
            \Statex $f$: 最终识别出的情感类别

        \Statex
        \State  计算$R$中不同情感类别出现的次数$N_e = \{n_{e_i}|e_i \in R\}$ % \State 后写一般语句
        \State  选出$N_e$中次数最多的情感类别，构成候选情感类别集合$C_{max} = \{c_k|c_k \in E; k = 1,2,...K\}$
        \State  $f := c_1$
    　　 \If{$K > 1$} % If 语句，需要和EndIf对应
    　　　　 \For{$k=2$ to $K$}
                \State $f := r_{fc_k}$
            \EndFor
    　　 \EndIf
        \State \Return $f$
    \end{algorithmic}
\end{algorithm}

投票决策是选取票数最多的情感类别作为最终的结果，但可能会存在出现多个相同最多票数的情感类别的情况。在本文采用投票决策算法中，会将所有最大票数的情感类别放入一个候选集合中，然后依次比较该候选集中两个情感所在情感对的二分类结果，保留胜出的那个情感类别，最后将会只剩下唯一一种情感类别。下面我们将作出形式化的证明：

\begin{proposition}
    投票决策算法在二分类结果正确的情况下一定可以得到正确的情感类别
\end{proposition}
\begin{proof}
    采用和算法~\ref{alg:voting}相同的符号表示，假设$e_i$是目标情感，则可以得到下面的证明过程：
    % \begin{equation*}
    \[
        \begin{aligned}
            R \quad is \quad correct &\Rightarrow n_{e_i} = M - 1 \\
            &\Rightarrow n_{e_j} < M - 1, e_j \in E - \{e_i\} \\
            &\Rightarrow f = e_i
        \end{aligned}
    \]
    % \end{equation*}
\end{proof}

从证明中可以看出当二分类结果是正确的时候，投票决策算法一定可以得到正确的情感类别，但通常我们无法保证所有二分类器的结果都是正确的，这会导致一些情感的票数相同。尽管在算法~\ref{alg:voting}中我们采取了一些策略保证最后可以得到唯一的情感，但是如果同票的几种情感的二分类结果间的胜出关系出现闭环，我们的投票策略同样无法保证得到的一定是最好的识别结果。假设有三种同票的情感$e_1$，$e_2$和$e_3$，$e_1>e_2$代表在$e_1$和$e_2$组成的情感对二分类结果中，$e_1$的概率更高。如果出现$e_1>e_2$，$e_2>e_3$，$e_3>e_1$这种情况，投票策略将无法做出有效的判决。此外，投票策略在判断最终的情感类别时是通过二分类器的结果决策，也就是说只有包含目标情感的那些二分类器才对最终的结果有好的影响，而其他二分类器的结果都不会对识别出正确的情感有好的影响。

\subsection{基于情感空间的贝叶斯决策融合}
鉴于上一节提到的关于基于投票的决策融合方式存在的问题，我们又提出了一种新的基于情感空间模型的贝叶斯决策融合来避免这些问题。我们在\ref{ssec:continuous_space}节提到过关于维度情感空间的定义，心理学上将情感通过一个笛卡尔空间坐标系来表示，其中每一个坐标轴都表示一种心理学属性，而不同的情感类别会被映射到空间中不同的位置。常用的情感空间模型有二维情感空间(激活度-效价)和三维情感空间(激活度-效价-支配力)，本文主要采用三维情感空间模型来描述。假设当前需要识别4种情感：高兴，悲伤，愤怒和中性，这些情感在维度情感空间中的分布大致如下图所示：

\begin{figure}[H] % use float package if you want it here
    \centering
    \includegraphics[height=10cm]{myfigures/2dim_space}
    \caption{激活度-效价情感空间模型}
    \label{fig:xfig1}
\end{figure}

从图中我们可以看出不同的情感类别之间的距离并不相同，例如高兴和愤怒的距离明显比高兴和悲伤的距离要小，而这种情感类别之间的距离恰好反映了它们之间的相似程度。回想一下日常生活中，当我们高兴地说话和愤怒地说话时，通常语气都比较急促，声音都比较大，但是当我们悲伤地说话时，声音一般都会比较缓慢而低沉。投票决策有个问题就是不包含目标情感的那些情感对的二分类结果对最后的决策并没有贡献，但现在有了维度情感空间的这种信息，我们可以认为这些不包含目标情感的情感对的二分类结果对最后的决策也是有帮助的。假设目标情感是高兴，对于愤怒-悲伤这个情感对来说，二分类结果是愤怒就更加支持最终的情感类别是高兴，是悲伤的话就更不支持最终的情感类别是高兴。这样的支持关系对其他的情感也是适用的，所以如果这种信息能够被有效地运用在决策融合当中，是有助于我们更好地判断最终情感类别的。

对于这种支持关系，需要通过一种数学模型来对其进行量化表示，最为自然的就是想到贝叶斯概率模型，因为这种支持关系可以通过条件概率来建模。假设我们需要识别的情感类别集合为$E=\{e_i|i=1, 2,...,M}$，情感对的所有二分类结果为$R = \{r_{e_ie_j}|e_i \neq e_j; r_{e_ie_j}, e_i, e_j \in E\}$。

\section{实验及结果分析}

\subsection{实验设置}

\subsection{实验结果分析}

\section{本章小结}